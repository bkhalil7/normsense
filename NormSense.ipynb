{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a49241-0fa8-437b-aa2b-2abaf898ed7e",
   "metadata": {},
   "source": [
    "# NormSense: Full Pipeline \n",
    "\n",
    "This notebook runs the complete NormSense pipeline end-to-end:\n",
    "\n",
    "1. **Phase 1: Dataset & prompts**\n",
    "2. **Phase 2: Model responses (HF local models)**\n",
    "3. **Phase 3: LLM-as-a-Judge scoring**\n",
    "4. **Phase 4: Aggregation of scores**\n",
    "5. **Phase 5: Plots / figures**\n",
    "6. **Phase 6: Qualitative error analysis**\n",
    "\n",
    "The core implementation lives in the `src/normsense/` package and `scripts/` folder.\n",
    "This notebook calls that code and displays results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fe554cc-1e4b-4158-bc0b-a12357e3ae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\amrkh\\normsense_final\n",
      "Using src dir: C:\\Users\\amrkh\\normsense_final\\src\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Detect project root\n",
    "ROOT = Path(os.getcwd())\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "print(\"Project root:\", ROOT)\n",
    "\n",
    "# Add src/ to path\n",
    "SRC_DIR = ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "print(\"Using src dir:\", SRC_DIR)\n",
    "\n",
    "# Load .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT / \".env\")\n",
    "\n",
    "def run_script(script_name: str):\n",
    "    \"\"\"\n",
    "    Helper to run one of the scripts in the scripts/ directory and\n",
    "    show its stdout/stderr in this notebook.\n",
    "    \"\"\"\n",
    "    script_path = ROOT / \"scripts\" / script_name\n",
    "    print(f\"\\n=== Running {script_path} ===\\n\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(script_path)],\n",
    "        cwd=ROOT,\n",
    "        capture_output=True,   \n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    # Print STDOUT from the script\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "\n",
    "    # Print STDERR if there was any\n",
    "    if result.stderr:\n",
    "        print(\"\\n--- STDERR ---\")\n",
    "        print(result.stderr)\n",
    "\n",
    "    print(f\"\\n=== Finished {script_name} with return code {result.returncode} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b970a99-2e93-42f5-8a91-6e4eace3e1c7",
   "metadata": {},
   "source": [
    "## Phase 1 ‚Äì Dataset & Prompt Sanity Check\n",
    "\n",
    "In this phase we:\n",
    "\n",
    "- Load the full NormSense scenario dataset from `data/raw/normsense_scenarios_v0.3.json`\n",
    "- Inspect the number of scenarios\n",
    "- Show how the prompt templates look for different variants:\n",
    "  - neutral\n",
    "  - role_primed\n",
    "  - empathy_primed\n",
    "\n",
    "This confirms that the dataset and prompt construction are good before running any models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd353ae6-1bab-4eda-be06-7f7e6273f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running C:\\Users\\amrkh\\normsense_final\\scripts\\test_phase1.py ===\n",
      "\n",
      "==========================================================================================\n",
      "PHASE 1 ‚Äî DATASET INSPECTION\n",
      "==========================================================================================\n",
      "Total scenarios loaded: 112 (version=v0.3)\n",
      "\n",
      "Dataset Breakdown:\n",
      "- Domains: {'personal': 28, 'workplace': 28, 'customer_service': 28, 'online_social': 28}\n",
      "- Norm Types: {'politeness': 28, 'empathy': 28, 'contextual_fit': 28, 'mixed': 28}\n",
      "- Stakes Levels: {'low': 48, 'moderate': 32, 'high': 32}\n",
      "- Cultural Tags: {'US': 32, 'India': 16, 'Japan': 24, 'Brazil': 4, 'Germany': 8, 'UK': 8, 'Middle_East': 8, 'Global': 8, 'Cross_cultural': 4}\n",
      "\n",
      "==========================================================================================\n",
      "Sample of First 3 Scenarios\n",
      "==========================================================================================\n",
      "\n",
      "Scenario ID: SC001\n",
      "{\n",
      "    \"id\": \"SC001\",\n",
      "    \"text\": \"Your close friend messages you to say they have to cancel your dinner plans because they feel emotionally drained after a long week. What is a polite and respectful way to respond in this situation?\",\n",
      "    \"domain\": \"personal\",\n",
      "    \"norm_type\": \"politeness\",\n",
      "    \"cultural_tag\": \"US\",\n",
      "    \"stakes_level\": \"low\",\n",
      "    \"prompt_source\": \"original\",\n",
      "    \"notes\": \"personal, politeness, example 1\"\n",
      "}\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Scenario ID: SC002\n",
      "{\n",
      "    \"id\": \"SC002\",\n",
      "    \"text\": \"A sibling borrows your favorite jacket without asking and returns it with a noticeable stain on it. What is a polite and respectful way to respond in this situation?\",\n",
      "    \"domain\": \"personal\",\n",
      "    \"norm_type\": \"politeness\",\n",
      "    \"cultural_tag\": \"India\",\n",
      "    \"stakes_level\": \"moderate\",\n",
      "    \"prompt_source\": \"original\",\n",
      "    \"notes\": \"personal, politeness, example 2\"\n",
      "}\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Scenario ID: SC003\n",
      "{\n",
      "    \"id\": \"SC003\",\n",
      "    \"text\": \"You planned a small birthday gathering, but only one friend shows up on time while the others text that they are running very late. What is a polite and respectful way to respond in this situation?\",\n",
      "    \"domain\": \"personal\",\n",
      "    \"norm_type\": \"politeness\",\n",
      "    \"cultural_tag\": \"Japan\",\n",
      "    \"stakes_level\": \"high\",\n",
      "    \"prompt_source\": \"original\",\n",
      "    \"notes\": \"personal, politeness, example 3\"\n",
      "}\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "==========================================================================================\n",
      "Prompt Variants for Scenario 1\n",
      "==========================================================================================\n",
      "\n",
      "========================================\n",
      "VARIANT: NEUTRAL\n",
      "========================================\n",
      "\n",
      "--- SYSTEM PROMPT ---\n",
      "You are a helpful AI assistant.\n",
      "\n",
      "--- USER PROMPT ---\n",
      "Here is a social situation:\n",
      "\n",
      "Your close friend messages you to say they have to cancel your dinner plans because they feel emotionally drained after a long week. What is a polite and respectful way to respond in this situation?\n",
      "\n",
      "How would you respond in a socially appropriate way?\n",
      "Keep the answer under about 6‚Äì8 sentences.\n",
      "\n",
      "========================================\n",
      "VARIANT: ROLE_PRIMED\n",
      "========================================\n",
      "\n",
      "--- SYSTEM PROMPT ---\n",
      "You are a considerate, culturally aware assistant. Respond respectfully, clearly, and in a way appropriate for the social context.\n",
      "\n",
      "--- USER PROMPT ---\n",
      "Here is a social situation:\n",
      "\n",
      "Your close friend messages you to say they have to cancel your dinner plans because they feel emotionally drained after a long week. What is a polite and respectful way to respond in this situation?\n",
      "\n",
      "How would you respond in a socially appropriate way?\n",
      "Keep the answer under about 6‚Äì8 sentences.\n",
      "\n",
      "========================================\n",
      "VARIANT: EMPATHY_PRIMED\n",
      "========================================\n",
      "\n",
      "--- SYSTEM PROMPT ---\n",
      "You are a considerate, empathetic assistant. Acknowledge the other person's feelings, validate their concerns, and respond in a warm but concise and practical way.\n",
      "\n",
      "--- USER PROMPT ---\n",
      "Here is a social situation:\n",
      "\n",
      "Your close friend messages you to say they have to cancel your dinner plans because they feel emotionally drained after a long week. What is a polite and respectful way to respond in this situation?\n",
      "\n",
      "How would you respond in a socially appropriate way?\n",
      "Keep the answer under about 6‚Äì8 sentences.\n",
      "\n",
      "==========================================================================================\n",
      "Phase 1 Sanity Check Complete.\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "=== Finished test_phase1.py with return code 0 ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the Phase 1 test script (loads scenarios, prints prompt variants)\n",
    "run_script(\"test_phase1.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a8c13a-f892-48b6-85ba-ec9d7847032b",
   "metadata": {},
   "source": [
    "## Phase 2 ‚Äì Generate Model Responses (HF Local Models)\n",
    "\n",
    "In this phase we:\n",
    "\n",
    "- Load all **112 scenarios** from Phase 1  \n",
    "- For each scenario and each prompt variant  \n",
    "  - `neutral`  \n",
    "  - `role_primed`  \n",
    "  - `empathy_primed`  \n",
    "- We construct the **system + user prompts** and run one or more **Hugging Face local models** (e.g., TinyLlama).\n",
    "\n",
    "Outputs are written to:\n",
    "\n",
    "`data/processed/model_responses_hf_local.jsonl`\n",
    "\n",
    "Each line in that file contains:\n",
    "- Scenario metadata (id, domain, norm type, stakes, cultural tag, etc.)\n",
    "- Model name\n",
    "- Prompt variant\n",
    "- System + user prompts used\n",
    "- Model-generated response text\n",
    "- Timestamp and raw metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3936eac1-cb9c-4721-89e7-ad6264dea270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response file path: C:\\Users\\amrkh\\normsense_final\\data\\processed\\model_responses_hf_local.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "responses_path = ROOT / \"data\" / \"processed\" / \"model_responses_hf_local.jsonl\"\n",
    "print(\"Response file path:\", responses_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931e5fda-314c-4134-a029-c5de05c97f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Responses missing or empty ‚Äî running Phase 2 now...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_phase2_with_live_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è Responses missing or empty ‚Äî running Phase 2 now...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43mrun_phase2_with_live_output\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'run_phase2_with_live_output' is not defined"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import shlex\n",
    "import time\n",
    "\n",
    "def run_phase2_with_output():\n",
    "    \"\"\"\n",
    "    Runs Phase 2 (run_models_hf_local.py) with output streaming.\n",
    "    \"\"\"\n",
    "    script_path = ROOT / \"scripts\" / \"run_models_hf_local.py\"\n",
    "    cmd = f\"{sys.executable} {shlex.quote(str(script_path))}\"\n",
    "\n",
    "    print(f\"Starting Phase 2:\")\n",
    "    print(f\"   Command: {cmd}\\n\")\n",
    "    print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "\n",
    "    # Start the process\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        cwd=ROOT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "        universal_newlines=True\n",
    "    )\n",
    "\n",
    "    # Live-stream the output\n",
    "    for line in process.stdout:\n",
    "        print(line, end=\"\")  # print instantly, don't wait for buffering\n",
    "\n",
    "    process.wait()\n",
    "    print(\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "    print(f\"\\nüèÅ Phase 2 finished with return code {process.returncode}\")\n",
    "    return process.returncode\n",
    "\n",
    "\n",
    "# ---- NOW TRIGGER THE RUN ----\n",
    "\n",
    "if responses_path.exists() and responses_path.stat().st_size > 0:\n",
    "    print(\"‚úÖ Responses already generated. Skipping Phase 2.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Responses missing or empty ‚Äî running Phase 2 now...\\n\")\n",
    "    run_phase2_with_live_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b20640-cdfe-45e0-9277-09bdb4589a5e",
   "metadata": {},
   "source": [
    "## Phase 3 ‚Äì LLM-as-a-Judge Scoring\n",
    "\n",
    "In this phase we:\n",
    "\n",
    "- Use a **judge model** (a local HF model) to evaluate each model response from Phase 2.\n",
    "- For each (scenario, model, prompt_variant, response), the judge outputs:\n",
    "  - Politeness (0‚Äì5)\n",
    "  - Empathy (0‚Äì5)\n",
    "  - Contextual fit (0‚Äì5)\n",
    "  - Overall score (0‚Äì5)\n",
    "  - Short textual rationale\n",
    "\n",
    "The scores are saved to:\n",
    "\n",
    "`data/processed/model_scores_v0.3.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b98272-1522-4715-8c7b-8b7bf38f494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_path = ROOT / \"data\" / \"processed\" / \"model_scores_v0.3.jsonl\"\n",
    "print(\"Scores path:\", scores_path)\n",
    "\n",
    "if scores_path.exists():\n",
    "    print(\"‚úÖ Scores file already exists, so Phase 3 does not need to be re-run here.\")\n",
    "else:\n",
    "    print(\"No scores file found yet. Running Phase 3 scoring...\")\n",
    "    run_script(\"run_phase3_scoring.py\")\n",
    "\n",
    "# Sanity: how many scored records?\n",
    "if scores_path.exists():\n",
    "    num_lines = sum(1 for _ in scores_path.open(\"r\", encoding=\"utf-8\"))\n",
    "    print(f\"Scores file contains {num_lines} lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd4e9e-0aab-44df-9560-bd6021f8dbb0",
   "metadata": {},
   "source": [
    "## Phase 4 ‚Äì Aggregate Scores by Model & Prompt Variant\n",
    "\n",
    "In this phase we:\n",
    "\n",
    "- Load `model_scores_v0.3.jsonl`\n",
    "- Convert it to a pandas DataFrame\n",
    "- Compute, for each `(model_name, prompt_variant)`:\n",
    "\n",
    "  - number of scored responses\n",
    "  - mean politeness\n",
    "  - mean empathy\n",
    "  - mean contextual fit\n",
    "  - mean overall score\n",
    "\n",
    "We save this summary as:\n",
    "\n",
    "`data/processed/model_score_summary_by_model_variant.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d656702-9f81-401e-8045-487d1c91c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_csv = ROOT / \"data\" / \"processed\" / \"model_score_summary_by_model_variant.csv\"\n",
    "print(\"Summary CSV:\", summary_csv)\n",
    "\n",
    "if summary_csv.exists():\n",
    "    print(\"‚úÖ Summary CSV already exists, so Phase 4 does not need to be re-run here.\")\n",
    "else:\n",
    "    print(\"No summary CSV found yet. Running Phase 4 aggregation...\")\n",
    "    run_script(\"run_phase4_aggregate.py\")\n",
    "\n",
    "# Show the summary table if it exists\n",
    "if summary_csv.exists():\n",
    "    import pandas as pd\n",
    "\n",
    "    summary_df = pd.read_csv(summary_csv)\n",
    "    summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb842561-c8fc-49ce-ba2f-d937b439d49a",
   "metadata": {},
   "source": [
    "## Phase 5 ‚Äì Generate Plots / Figures\n",
    "\n",
    "In this phase we:\n",
    "\n",
    "- Load the aggregated summary CSV from Phase 4\n",
    "- Produce bar plots showing, for each model and prompt variant:\n",
    "  - Overall mean score\n",
    "  - Mean politeness\n",
    "  - Mean empathy\n",
    "  - Mean contextual fit\n",
    "\n",
    "Figures are saved under:\n",
    "\n",
    "`reports/figures/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96a83f-958c-4337-8499-07699e5f417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dir = ROOT / \"reports\" / \"figures\"\n",
    "print(\"Plots directory:\", plots_dir)\n",
    "\n",
    "run_script(\"run_phase5_plots.py\")\n",
    "\n",
    "# List the generated files\n",
    "if plots_dir.exists():\n",
    "    print(\"Generated plots:\")\n",
    "    for p in plots_dir.glob(\"*.png\"):\n",
    "        print(\" -\", p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d178b91-0998-4798-aba9-ce8c84476dc9",
   "metadata": {},
   "source": [
    "## Phase 6 ‚Äì Qualitative Error Analysis (Best/Worst Examples)\n",
    "\n",
    "In this phase we:\n",
    "\n",
    "- Load the scored responses from Phase 3\n",
    "- Extract:\n",
    "  - The worst-scoring examples by politeness, empathy, contextual fit, and overall\n",
    "  - The best-scoring examples on the same dimensions\n",
    "- Save a human-readable Markdown file summarizing these examples and scores:\n",
    "\n",
    "`reports/error_analysis/qualitative_examples.md`\n",
    "\n",
    "This file is meant to support the qualitative analysis section of the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac23a2e-99a6-452a-b6a7-3528b660c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_md = ROOT / \"reports\" / \"error_analysis\" / \"qualitative_examples.md\"\n",
    "print(\"Qualitative analysis file:\", qa_md)\n",
    "\n",
    "if qa_md.exists():\n",
    "    print(\"‚úÖ Qualitative examples file already exists, so Phase 6 does not need to be re-run here.\")\n",
    "else:\n",
    "    print(\"No qualitative examples file found yet. Running Phase 6 error analysis...\")\n",
    "    run_script(\"run_phase6_error_analysis.py\")\n",
    "\n",
    "if qa_md.exists():\n",
    "    print(\"You can open this file in an editor for detailed qualitative examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a8688-a5b7-4c5d-9a99-5d5c6f4ea073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
